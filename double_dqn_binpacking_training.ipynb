{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83c\udf93 Train Double DQN for 2D Bin Packing\n", "This notebook trains a Double DQN model with a corrected reward function to avoid unrealistic placements."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u2705 Install dependencies\n", "!pip install tensorflow numpy matplotlib"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u2705 Environment and agent setup\n", "import numpy as np\n", "import random\n", "from collections import deque\n", "import matplotlib.pyplot as plt\n", "from tensorflow.keras.models import Sequential, clone_model\n", "from tensorflow.keras.layers import Dense, Input\n", "from tensorflow.keras.models import load_model\n", "\n", "class BinPacking2DEnv:\n", "    def __init__(self, bin_width=1.0, bin_height=1.0, num_bins=5, items_per_episode=20):\n", "        self.bin_width = bin_width\n", "        self.bin_height = bin_height\n", "        self.num_bins = num_bins\n", "        self.items_per_episode = items_per_episode\n", "        self.reset()\n", "\n", "    def reset(self):\n", "        self.bins = [[] for _ in range(self.num_bins)]\n", "        self.items = [tuple(np.random.uniform(0.1, 0.4, size=2)) for _ in range(self.items_per_episode)]\n", "        self.current_index = 0\n", "        self.current_item = self.items[self.current_index]\n", "        return self.get_state()\n", "\n", "    def _can_place(self, bin_items, item):\n", "        y_offset = sum(h for _, h in bin_items)\n", "        return y_offset + item[1] <= self.bin_height and item[0] <= self.bin_width\n", "\n", "    def step(self, action):\n", "        reward = -1\n", "        item = self.current_item\n", "        if self._can_place(self.bins[action], item):\n", "            self.bins[action].append(item)\n", "            reward = 1\n", "        else:\n", "            reward = -5  # \u2757 Strong penalty for illegal placement\n", "        self.current_index += 1\n", "        done = self.current_index >= self.items_per_episode\n", "        if not done:\n", "            self.current_item = self.items[self.current_index]\n", "        return self.get_state(), reward, done\n", "\n", "    def get_state(self):\n", "        flat = [d for bin in self.bins for r in bin for d in r][:30]\n", "        padded = flat + [0.0] * (30 - len(flat))\n", "        return np.array(list(self.current_item) + padded)\n", "\n", "    def get_bins_used(self):\n", "        return sum(1 for b in self.bins if b)\n", "\n", "class DoubleDQNAgent:\n", "    def __init__(self, state_size, action_size):\n", "        self.state_size = state_size\n", "        self.action_size = action_size\n", "        self.memory = deque(maxlen=2000)\n", "        self.gamma = 0.95\n", "        self.epsilon = 1.0\n", "        self.epsilon_min = 0.01\n", "        self.epsilon_decay = 0.995\n", "        self.model = self._build_model()\n", "        self.target_model = clone_model(self.model)\n", "        self.target_model.set_weights(self.model.get_weights())\n", "\n", "    def _build_model(self):\n", "        model = Sequential()\n", "        model.add(Input(shape=(self.state_size,)))\n", "        model.add(Dense(128, activation='relu'))\n", "        model.add(Dense(128, activation='relu'))\n", "        model.add(Dense(self.action_size, activation='linear'))\n", "        model.compile(optimizer='adam', loss='mse')\n", "        return model\n", "\n", "    def act(self, state):\n", "        if np.random.rand() <= self.epsilon:\n", "            return random.randrange(self.action_size)\n", "        q_values = self.model.predict(state, verbose=0)\n", "        return np.argmax(q_values[0])\n", "\n", "    def remember(self, s, a, r, s_, done):\n", "        self.memory.append((s, a, r, s_, done))\n", "\n", "    def replay(self, batch_size):\n", "        minibatch = random.sample(self.memory, batch_size)\n", "        for s, a, r, s_, done in minibatch:\n", "            target = self.model.predict(s, verbose=0)\n", "            if done:\n", "                target[0][a] = r\n", "            else:\n", "                next_a = np.argmax(self.model.predict(s_, verbose=0)[0])\n", "                t_q = self.target_model.predict(s_, verbose=0)[0][next_a]\n", "                target[0][a] = r + self.gamma * t_q\n", "            self.model.fit(s, target, epochs=1, verbose=0)\n", "        if self.epsilon > self.epsilon_min:\n", "            self.epsilon *= self.epsilon_decay\n", "\n", "    def update_target(self):\n", "        self.target_model.set_weights(self.model.get_weights())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u2705 Train Double DQN Agent\n", "env = BinPacking2DEnv()\n", "agent = DoubleDQNAgent(len(env.get_state()), env.num_bins)\n", "\n", "episodes = 100\n", "for e in range(episodes):\n", "    state = env.reset().reshape(1, -1)\n", "    done = False\n", "    while not done:\n", "        action = agent.act(state)\n", "        next_state, reward, done = env.step(action)\n", "        agent.remember(state, action, reward, next_state.reshape(1, -1), done)\n", "        state = next_state.reshape(1, -1)\n", "    if len(agent.memory) >= 32:\n", "        agent.replay(32)\n", "    if e % 10 == 0:\n", "        agent.update_target()\n", "    print(f\"Episode {e+1} completed\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u2705 Save model\n", "agent.model.save(\"double_dqn_episode_final.keras\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11"}}, "nbformat": 4, "nbformat_minor": 2}